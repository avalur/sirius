{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/logo_fmkn.png\" alt=\"logo_fmkn\" />\n",
    "</div>\n",
    "\n",
    "# Введение в рекуррентные нейронные сети\n",
    "\n",
    "### Занятие 4. Long short-term memory (LSTM)\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "19 января 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посмотрим и обсудим визуализацию работы RNN:\n",
    "\n",
    "https://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Глубокие рекуррентные сети\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"center\">\n",
    "    <b>Vanilla RNN</b>\n",
    "    $$h_t^\\ell = \\tanh W^\\ell \\left(\\begin{array}{c}\n",
    "    h_t^{\\ell-1} \\\\ h_{t-1}^{\\ell}\n",
    "    \\end{array}\\right) \\\\\n",
    "    h \\in \\mathbb{R}^n, \\quad\\quad W^\\ell [n \\times 2n] $$\n",
    "    <hr>\n",
    "    <b>LSTM</b>\n",
    "    $$\\quad\\quad W^\\ell [4n\\times 2n]\\\\\n",
    "    \\left(\\begin{array}{c}\n",
    "    i \\\\ f \\\\ o \\\\ c_t^\\prime\n",
    "    \\end{array}\\right) = \n",
    "    \\left(\\begin{array}{c}\n",
    "    \\text{sigm} \\\\ \\text{sigm} \\\\ \\text{sigm} \\\\ \\tanh\n",
    "    \\end{array}\\right)\n",
    "    W^\\ell \\left(\\begin{array}{c}\n",
    "    h_t^{\\ell-1} \\\\ h_{t-1}^{\\ell}\n",
    "    \\end{array}\\right)\\\\\n",
    "    \\begin{array}{l}\n",
    "    c_t^\\ell = f \\odot c_{t-1}^\\ell + i \\odot c_t^\\prime \\\\\n",
    "    h_t^\\ell = o \\odot \\tanh(c_t^\\ell)\n",
    "    \\end{array} \\\\\n",
    "    \\odot - \\text{ покомпонентное произведение}\n",
    "    $$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><img src=\"images/rnn_depth.jpg\" alt=\"rnn_architectures\"/></td>  \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Мотивация и схема LSTM\n",
    "\n",
    "Сеть должна долго помнить контекст. Какой именно сеть выучивает сама.\n",
    "\n",
    "Для этого вводится вектор $c_t$ — вектор состояния сети в момент $t$.\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ c_t^\\prime = \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_{c^\\prime}) \\\\\n",
    "    i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_{i}) \\\\\n",
    "    f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_{f}) \\\\\n",
    "    o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_{o}) \\\\\n",
    "    c_t = f_t \\odot c_{t-1} + i_t \\odot c_t^\\prime \\\\\n",
    "    h_t = o_t \\odot \\tanh(c_t)$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ candidate\\ cell\\ state \\\\\n",
    "      input\\ gate \\\\\n",
    "      forget\\ gate \\\\\n",
    "      output\\ gate \\\\\n",
    "      cell\\ state \\\\\n",
    "      block\\ output$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/lstm_module.jpg\" alt=\"lstm_module\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM: forget gate\n",
    "\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "      \\ \\\\\n",
    "    f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_{f}) \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "      \\ \\\\\n",
    "      forget\\ gate \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/forget_gate.jpg\" alt=\"forget_gate\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Пример: удаление пола действующего лица при генерации текста\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM: input/candidate gates\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ c_t^\\prime = \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_{c^\\prime}) \\\\\n",
    "    i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_{i}) \\\\\n",
    "    \\ \\\\\n",
    "    \\ \\\\\n",
    "    \\ \\\\\n",
    "    \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ candidate\\ cell\\ state \\\\\n",
    "      input\\ gate \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/input_gate.jpg\" alt=\"input_gate\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Пример: добавление пола действующего лица при генерации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM: обновление состояния ячейки памяти\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "    \\ \\\\\n",
    "    \\ \\\\\n",
    "    \\ \\\\\n",
    "    c_t = f_t \\odot c_{t-1} + i_t \\odot c_t^\\prime \\\\\n",
    "    \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      cell\\ state \\\\\n",
    "      \\ $\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/lstm_update.jpg\" alt=\"lstm_update\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM: генерация выхода\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "    \\ \\\\\n",
    "    \\ \\\\\n",
    "    o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_{o}) \\\\\n",
    "    \\ \\\\\n",
    "    h_t = o_t \\odot \\tanh(c_t)$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ \\ \\\\\n",
    "      \\ \\\\\n",
    "      \\ \\\\\n",
    "      output\\ gate \\\\\n",
    "      \\ \\\\\n",
    "      block\\ output$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/lstm_output.jpg\" alt=\"lstm_output\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GRU: Gated Recurrent Unit\n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "    <div align=\"left\">\n",
    "    $ u_t = \\sigma(W_{xu}x_t + W_{hu}h_{t-1} + b_{u}) \\\\\n",
    "      r_t = \\sigma(W_{xr}x_t + W_{hr}h_{t-1} + b_{r}) \\\\\n",
    "      h_t^\\prime = \\tanh(W_{xh^\\prime}x_t + W_{hh^\\prime}(r_t\\odot h_{t-1})) \\\\\n",
    "      h_t = (1-u_t) \\odot h_t^\\prime + u_t \\odot h_{t-1}$\n",
    "    </div>\n",
    "    </td>\n",
    "    <td><div align=\"center\">\n",
    "        <img src=\"images/gru.jpg\" alt=\"gru_module\"/>\n",
    "        </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RNN для синтеза последовательностей (seq2seq)\n",
    "\n",
    "$X = (x_1, \\dots, x_n)$ — входная последовательность\n",
    "\n",
    "$Y = (y_1, \\dots, y_m)$ — выходная последовательность\n",
    "\n",
    "$\\color{red}{c \\equiv h_n}$ кодирует всю информацию про $X$ для синтеза $Y$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/seq2seq.png\" alt=\"seq2seq\" width=400/>\n",
    "</div>\n",
    "\n",
    "$h_i = f_{in}(x_i, h_{i-1})$\n",
    "\n",
    "$\\color{red}{h_t^\\prime = f_{out}(h_{t-1}^\\prime,y_{t-1},c)}$\n",
    "\n",
    "$y_t = f_{y}(h_t^\\prime, y_{t-1})$\n",
    "\n",
    " * $h_n$ лучше помнит конец последовательности, чем начало\n",
    " * чем больше $n$, тем труднее упаковать всю информацию в $c$\n",
    " * придётся контролировать затухание\\взрывы градиента\n",
    " * RNN трудно распараллеливается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RNN с вниманием (attention mechanism)\n",
    "\n",
    "$a(h, h^\\prime)$ — функция сходства состояний входа $h$ и выхода $h^\\prime$ \n",
    "(скалярное произведение или $\\exp(h^T h^\\prime)$ и другие) \n",
    "\n",
    "$\\alpha_{ti}$ — важность входа $i$ для выхода $t$ (attention score), $\\sum_i \\alpha_{ti} = 1$\n",
    "\n",
    "$c_t$ — вектор входного контекста для выхода $t$ (context vector)\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/seq2seq_attention.png\" alt=\"seq2seq_attention\" width=400/>\n",
    "</div>\n",
    "\n",
    "$h_i = f_{in}(x_i, h_{i-1})$\n",
    "\n",
    "$\\color{red}{\\alpha_{ti} = \\text{norm}_i a(h_i, h^\\prime_{t-1})},\\ \\text{norm}_i(p) = \\frac{p_i}{\\sum_j p_j} \\\\\n",
    "\\color{red}{c_t = \\sum_i \\alpha_{ti} h_i}$\n",
    "\n",
    "$h_t^\\prime = f_{out} (h^\\prime_{t-1}, y_{t-1}, \\color{red}{c_t}) \\\\ y_t = f_{y}(h_t^\\prime, y_{t-1}, \\color{red}{c_t})$\n",
    "\n",
    " * можно вводить обучаемые параметры в $a$ и $c$\n",
    " * можно отказаться от рекуррентности как по $h_i$, так и по $h_t^\\prime$\n",
    " \n",
    "------\n",
    "_Bahdanau et al._ Neural machine translation by jointly learning to align and translate. 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Резюме\n",
    "\n",
    " * Рекуррентные нейронные сети — простой, мощный и гибкий подход решения различных задач машинного обучения\n",
    " * Vanilla RNN просты, но всё-таки недостаточно хороши\n",
    " * Поэтому нужно использовать LSTM или GRU\n",
    " * Зануление градиентов предотвращает LSTM\n",
    " * А от «взрыва градиента» помогает clipping\n",
    " * По-прежнему необходимо более глубокое понимание, как теоретическое, так и практическое\n",
    "   \n",
    "### Что ещё можно посмотреть?\n",
    " * Лекция 10 курса «CS231n» Andrej Karpathy в Стенфорде: https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    " * Как тренировать нейросети? http://karpathy.github.io/2019/04/25/recipe/\n",
    " * Аналогичное занятие курса Школы анализа данных: https://github.com/yandexdataschool/Practical_DL/tree/fall21/week06_rnn\n",
    " * Лекция К.В. Воронцова «Модели внимания и трансформеры» https://www.youtube.com/watch?v=KhMweP00S44\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
